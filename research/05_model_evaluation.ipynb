{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f:\\\\Project\\\\Text_Summarization\\\\research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f:\\\\Project\\\\Text_Summarization'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen= True)\n",
    "class ModelEvaluationConfig:\n",
    "    root_dir: Path\n",
    "    dataset_path: Path\n",
    "    model_ckpt: Path\n",
    "    tokenizer_path: Path\n",
    "    metric_file_name: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textSummarizer.constants import *\n",
    "from textSummarizer.utils.common import read_yaml, create_directories\n",
    "\n",
    "class ConfigManager:\n",
    "    def __init__(self, config_path = CONFIG_FILE_PATH, params_path = PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_path)\n",
    "        self.params = read_yaml(params_path)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "    \n",
    "    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        config = self.config.model_evaluation\n",
    "        model_eval_config = ModelEvaluationConfig(\n",
    "            root_dir= config.root_dir,\n",
    "            dataset_path= os.path.join(\n",
    "                self.config.data_transformation.root_dir,\n",
    "                self.config.dataset_name + '_dataset'),\n",
    "            model_ckpt= os.path.join(self.config.model_trainer.root_dir, 'model'),\n",
    "            tokenizer_path= os.path.join(self.config.model_trainer.root_dir, 'tokenizer'),\n",
    "            metric_file_name  = config.metric_file_name\n",
    "        )\n",
    "        return model_eval_config\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textSummarizer.logging import logger\n",
    "from datasets import load_dataset, load_from_disk, load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "from tqdm import tqdm \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluation:\n",
    "    def __init__(self,config: ModelEvaluationConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def generate_batch_sized_chunks(self, list_of_elements, batch_size):\n",
    "        for i in range(0, len(list_of_elements), batch_size):\n",
    "            yield list_of_elements[i: i+batch_size]\n",
    "            \n",
    "    def calculate_metric_on_test_dataset(self, dataset, tokenizer: AutoTokenizer, model: AutoModelForSeq2SeqLM, \n",
    "                                         metric, batch_size: int = 16,\n",
    "                                         device = 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "                                         text_column= 'dialogue', summary_column = 'summary'):\n",
    "        x_batches = list(self.generate_batch_sized_chunks(dataset[text_column], batch_size))\n",
    "        y_batches = list(self.generate_batch_sized_chunks(dataset[summary_column], batch_size))\n",
    "\n",
    "        for x_batch, y_batch in tqdm(zip(x_batches, y_batches), total = len(x_batches)):\n",
    "            inputs = tokenizer(x_batch, max_length = 1024, padding = 'max_length', truncation = True, return_tensors = 'pt')\n",
    "            summaries = model.generate(input_ids = inputs['input_ids'].to(device), attention_mask = inputs['attention_mask'].to(device),\n",
    "                                     length_penalty = 0.8, num_beams = 8, max_length = 128)\n",
    "            decoded_summaries = [tokenizer.decode(s, skip_special_tokens = True, clean_up_tokenization_spaces = True) for s in summaries]\n",
    "            decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]\n",
    "            metric.add_batch(predictions=decoded_summaries, references=y_batch)\n",
    "            \n",
    "        score = metric.compute()\n",
    "        return score\n",
    "    \n",
    "    def evaluate(self):\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(self.config.model_ckpt).to(device)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.config.tokenizer_path)\n",
    "\n",
    "        datset = load_from_disk(os.path.join(self.config.dataset_path, 'test'))\n",
    "\n",
    "        rouge_name = ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']\n",
    "        rouge_metric = load_metric('rouge')\n",
    "        score  = self.calculate_metric_on_test_dataset(datset[0:5], tokenizer, model, rouge_metric, 2, device, 'dialogue', 'summary')\n",
    "\n",
    "        rouge_dict = ((rn, score[rn].mid.fmeasure) for rn in rouge_name)\n",
    "\n",
    "        df = pd.DataFrame(rouge_dict, index = 'pegasus')\n",
    "\n",
    "        df.to_csv(self.config.metric_file_name, index= False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-16 22:03:43,772 : INFO : common : yaml file : config\\config.yml loaded succefully]\n",
      "[2024-01-16 22:03:43,819 : INFO : common : yaml file : params.yml loaded succefully]\n",
      "[2024-01-16 22:03:43,850 : INFO : common : Created Director at: artifacts]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [06:07<00:00, 122.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-16 22:15:55,406 : INFO : rouge_scorer : Using default tokenizer.]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'rougeLSum'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m     model_evaluation\u001b[38;5;241m.\u001b[39mevaluate()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[1;32mIn[30], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m     model_evaluation_config \u001b[38;5;241m=\u001b[39m config_manager\u001b[38;5;241m.\u001b[39mget_model_evaluation_config()\n\u001b[0;32m      4\u001b[0m     model_evaluation \u001b[38;5;241m=\u001b[39m ModelEvaluation(model_evaluation_config)\n\u001b[1;32m----> 5\u001b[0m     model_evaluation\u001b[38;5;241m.\u001b[39mevaluate()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[1;32mIn[29], line 40\u001b[0m, in \u001b[0;36mModelEvaluation.evaluate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     36\u001b[0m score  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_metric_on_test_dataset(datset[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m5\u001b[39m], tokenizer, model, rouge_metric, \u001b[38;5;241m2\u001b[39m, device, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdialogue\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     38\u001b[0m rouge_dict \u001b[38;5;241m=\u001b[39m ((rn, score[rn]\u001b[38;5;241m.\u001b[39mmid\u001b[38;5;241m.\u001b[39mfmeasure) \u001b[38;5;28;01mfor\u001b[39;00m rn \u001b[38;5;129;01min\u001b[39;00m rouge_name)\n\u001b[1;32m---> 40\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(rouge_dict, index \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpegasus\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     42\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmetric_file_name, index\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Omkar\\anaconda3\\envs\\text_summerization\\Lib\\site-packages\\pandas\\core\\frame.py:798\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    796\u001b[0m         data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(data)\n\u001b[0;32m    797\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 798\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data)\n\u001b[0;32m    799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    800\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_dataclass(data[\u001b[38;5;241m0\u001b[39m]):\n",
      "Cell \u001b[1;32mIn[29], line 38\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     35\u001b[0m rouge_metric \u001b[38;5;241m=\u001b[39m load_metric(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrouge\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     36\u001b[0m score  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_metric_on_test_dataset(datset[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m5\u001b[39m], tokenizer, model, rouge_metric, \u001b[38;5;241m2\u001b[39m, device, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdialogue\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 38\u001b[0m rouge_dict \u001b[38;5;241m=\u001b[39m ((rn, score[rn]\u001b[38;5;241m.\u001b[39mmid\u001b[38;5;241m.\u001b[39mfmeasure) \u001b[38;5;28;01mfor\u001b[39;00m rn \u001b[38;5;129;01min\u001b[39;00m rouge_name)\n\u001b[0;32m     40\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(rouge_dict, index \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpegasus\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     42\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmetric_file_name, index\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'rougeLSum'"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config_manager = ConfigManager()\n",
    "    model_evaluation_config = config_manager.get_model_evaluation_config()\n",
    "    model_evaluation = ModelEvaluation(model_evaluation_config)\n",
    "    model_evaluation.evaluate()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_summerization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
